{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Flatten,Dropout, LSTM, BatchNormalization,SimpleRNN,Input,Conv1D,Conv2D,Concatenate,Activation,MaxPooling1D,Masking,GRU,Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "\n",
    "import csv\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Local\\Temp\\ipykernel_70056\\3896751810.py:1: DtypeWarning: Columns (172) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\Data_outcome_deidentified.csv\", header=0)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\Data_outcome_deidentified.csv\", header=0)\n",
    "df.drop(['POST','PRE','causeofdeath','primcauseofdeath','death.info','numdisvessels','maxlesionlength', 'wposttimi', 'numbms','numdes', 'numstents', 'maxdevdiameter', 'maxdevlength','otherventsupport\\nYes:1 \\nNo:0'],axis=1,inplace=True)\n",
    "effect = ['Inhospital mortality\\nYes:1\\nNo:0','Mortality 6 months\\nYes:1\\nNo:0', 'Mortality 1 yr\\nYes:1\\nNo:0','Mortality 2 yr\\nYes:1\\nNo:0', 'Mortality 5yr\\nYes:1\\nNo:0','Time from PCI to Stroke_6mo', 'Time from PCI to Stroke_1yr','Time from PCI to Stroke_2yr', 'Time from PCI to Stroke_5yr']\n",
    "causes =  df.drop(effect,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conf = pd.read_csv(r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\test_confidence.csv\", header=0)\n",
    "test_conf = test_conf.drop(['Unnamed: 0'], axis=1)\n",
    "train_conf = pd.read_csv(r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\train_confidence.csv\", header=0)\n",
    "train_conf = train_conf.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "f1_cutoff = [0.618908763,0.874658227,0.84739238,0.341690063,0.216099053621292,0.228994444,0.009523808,0,0]\n",
    "cf_cutoff = [0.4,0.413742632,0.257319182,0.271106124,0.254969954,0,0,0,0]\n",
    "\n",
    "train_best = [0.54, 0.4, 0.28, 0.0, 0.24, 0, 0, 0, 0]\n",
    "test_best = [0.02, 0.1, 0.3, 0.16, 0.14, 0, 0, 0, 0]\n",
    "\n",
    "test_ranges = [6365, 5931, 5211, 4348, 3179, 5599, 5072, 4154, 2104]\n",
    "train_ranges = [6364, 5930, 5210, 4347, 3179, 5598, 5071, 4153, 2104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cutoffs = []\n",
    "train_cutoffs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, tn, fp, fn):\n",
    "    if (tp + fn) == 0:\n",
    "        return 0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def precision(tp, tn, fp, fn):\n",
    "    if (tp + fp) == 0:\n",
    "        return 0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def f1(precision, recall):\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleCutoff(data, range, cutoff, eff):\n",
    "    results = []\n",
    "    data = data.iloc[:range:]\n",
    "\n",
    "    for j in data[eff]:\n",
    "        if cf_cutoff[count] == 0:\n",
    "            results.append(0)\n",
    "        elif j > cutoff:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleCompare(data, array):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    counter = 0\n",
    "    for x in array:\n",
    "        if x == 1 and data[counter] == 1:\n",
    "            tp += 1\n",
    "        elif x == 1 and data[counter] == 0:\n",
    "            fn += 1\n",
    "        elif x == 0 and data[counter] == 1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "        counter += 1\n",
    "    \n",
    "    return tp, tn, fp, fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_cf = []\n",
    "best_test_cf = []\n",
    "count = 0\n",
    "for e in effect:\n",
    "    #print(e, \"\\n\")\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    best_train_score = 0\n",
    "    best_train_cut = 0\n",
    "    best_test_score = 0\n",
    "    best_test_cut = 0\n",
    "    for i in range(51):\n",
    "        cutoff = i/50\n",
    "        train_data = singleCutoff(train_conf, train_ranges[count], cutoff, e)\n",
    "        test_data = singleCutoff(test_conf, test_ranges[count], cutoff, e)\n",
    "        train_tp, train_tn, train_fp, train_fn = singleCompare(train_data, y_trains[count])\n",
    "        test_tp, test_tn, train_fp, train_fn = singleCompare(test_data, y_tests[count])\n",
    "        train_precision = precision(train_tp, train_tn, train_fp, train_fn)\n",
    "        train_recall = recall(train_tp, train_tn, train_fp, train_fn)\n",
    "        test_precision = precision(test_tp, test_tn, train_fp, train_fn)\n",
    "        test_recall = recall(test_tp, test_tn, train_fp, train_fn)\n",
    "        train_f1 = f1(train_precision, train_recall)\n",
    "        test_f1 = f1(test_precision, test_recall)\n",
    "        \n",
    "\n",
    "        if train_f1 > best_train_score:\n",
    "            best_train_score = train_f1\n",
    "            best_train_cut = cutoff\n",
    "\n",
    "        if test_f1 > best_test_score:\n",
    "            best_test_score = test_f1\n",
    "            best_test_cut = cutoff\n",
    "\n",
    "    best_train_cf.append(best_train_cut)\n",
    "    best_test_cf.append(best_test_cut)\n",
    "        \n",
    "    count += 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54, 0.4, 0.28, 0.0, 0.24, 0, 0, 0, 0]\n",
      "[0.02, 0.1, 0.3, 0.16, 0.14, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(best_train_cf)\n",
    "print(best_test_cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCutoffs(test_data,ranges,cut_scores):\n",
    "    saved_data = test_data\n",
    "    test_results = []\n",
    "    count = 0\n",
    "\n",
    "    for e in effect:\n",
    "        test_results.append([])\n",
    "\n",
    "        test_data = saved_data.iloc[:ranges[count]:]\n",
    "\n",
    "        for j in test_data[e]:\n",
    "            if cut_scores[count] == 0:\n",
    "                test_results[count].append(0)\n",
    "            elif j > cut_scores[count]:\n",
    "                test_results[count].append(1)\n",
    "            else:\n",
    "                test_results[count].append(0)\n",
    "        count += 1\n",
    "\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cutoffs = getCutoffs(test_conf,test_ranges,test_best)\n",
    "train_cutoffs = getCutoffs(train_conf,train_ranges,train_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6364\n",
      "5930\n",
      "5210\n",
      "4347\n",
      "2543\n",
      "5598\n",
      "5071\n",
      "4153\n",
      "2104\n"
     ]
    }
   ],
   "source": [
    "y_trains = []\n",
    "y_tests = []\n",
    "count = 0\n",
    "for e in effect:\n",
    "    X= df[df[e]>-1]\n",
    "    \n",
    "    if(e =='Mortality 1 yr\\nYes:1\\nNo:0'):\n",
    "        X=X[X['Mortality 6 months\\nYes:1\\nNo:0']!=1]\n",
    "    elif(e =='Mortality 2 yr\\nYes:1\\nNo:0' ):\n",
    "        X=X[(X['Mortality 6 months\\nYes:1\\nNo:0']!=1) & (X['Mortality 1 yr\\nYes:1\\nNo:0']!=1)]\n",
    "    elif(e =='Mortality 5yr\\nYes:1\\nNo:0'):\n",
    "        X = X[(X['Mortality 6 months\\nYes:1\\nNo:0']!=1) & (X['Mortality 1 yr\\nYes:1\\nNo:0']!=1) & (X['Mortality 2 yr\\nYes:1\\nNo:0']!=1) ]\n",
    "    if(e =='Time from PCI to Stroke_1yr'):\n",
    "        X=X[X['Time from PCI to Stroke_6mo']!=1]\n",
    "    elif(e =='Time from PCI to Stroke_2yr' ):\n",
    "        X=X[(X['Time from PCI to Stroke_6mo']!=1) & (X['Time from PCI to Stroke_1yr']!=1)]\n",
    "    elif(e =='Time from PCI to Stroke_5yr'):\n",
    "        X = X[(X['Time from PCI to Stroke_6mo']!=1) & (X['Time from PCI to Stroke_1yr']!=1) & (X['Time from PCI to Stroke_2yr']!=1) ]\n",
    "\n",
    "\n",
    "    y= X[e]\n",
    "    X=X.drop(effect,axis=1)\n",
    "    #print(y.iloc[:100])\n",
    "    #X_train,X_test = X.iloc[:len(X)//2,:],X.iloc[len(X)//2:,:]\n",
    "    y_train,y_test = y.iloc[:len(y)//2-1],y.iloc[len(y)//2+1:]\n",
    "    y_trains.append(y_train)\n",
    "    y_tests.append(y_test)\n",
    "    print(len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareResults(newData, y_array):\n",
    "    result = []\n",
    "    for i in range(9):\n",
    "        result.append([])\n",
    "        count = 0\n",
    "        for x in y_array[i]:\n",
    "            if x == 1 and newData[i][count] == 1:\n",
    "                result[i].append('tp')\n",
    "            elif x == 1 and newData[i][count] == 0:\n",
    "                result[i].append('fn')\n",
    "            elif x == 0 and newData[i][count] == 1:\n",
    "                result[i].append('fp')\n",
    "            else:\n",
    "                result[i].append('tn')\n",
    "            count += 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = compareResults(test_cutoffs, y_tests)\n",
    "train_results = compareResults(train_cutoffs, y_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(data, ranges):\n",
    "    saved_data = causes\n",
    "    count = 0\n",
    "    true_positives = []\n",
    "    true_negatives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    for e in effect:\n",
    "        true_positives.append([])\n",
    "        true_negatives.append([])\n",
    "        false_positives.append([])\n",
    "        false_negatives.append([])\n",
    "        for i in causes:\n",
    "            tp_count = 0\n",
    "            tn_count = 0\n",
    "            fp_count = 0\n",
    "            fn_count = 0\n",
    "            total = 0\n",
    "\n",
    "            comp_data = saved_data.iloc[:ranges[count]:]\n",
    "\n",
    "            #print(ranges[count])\n",
    "            #print(len(comp_data))\n",
    "            #print(len(data[count]))\n",
    "\n",
    "            index = 0\n",
    "            for x in comp_data[i]:\n",
    "                if x == 1:\n",
    "                    if data[count][index] == 'tp':\n",
    "                        tp_count += 1\n",
    "                    if data[count][index] == 'tn':\n",
    "                        tn_count += 1\n",
    "                    if data[count][index] == 'fp':\n",
    "                        fp_count += 1\n",
    "                    if data[count][index] == 'fn':\n",
    "                        fn_count += 1 \n",
    "                    total += 1\n",
    "                index += 1\n",
    "            \n",
    "            if total > 0:\n",
    "                tps = tp_count / total\n",
    "                tns = tn_count / total\n",
    "                fps = fp_count / total\n",
    "                fns = fn_count / total\n",
    "\n",
    "            #print(tps, tns, fps, fns)\n",
    "\n",
    "            true_positives[count].append(tps)\n",
    "            true_negatives[count].append(tns)\n",
    "            false_positives[count].append(fps)\n",
    "            false_negatives[count].append(fns)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return true_positives, true_negatives, false_positives, false_negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tp, test_tn, test_fp, test_fn = probability(test_results, test_ranges)\n",
    "train_tp, train_tn, train_fp, train_fn = probability(train_results, train_ranges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause = []\n",
    "for c in causes:\n",
    "    cause.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob = pd.DataFrame()\n",
    "test_prob = pd.DataFrame()\n",
    "train_prob['names'] = cause\n",
    "test_prob['names'] = cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpn = 'tp'\n",
    "tnn = 'tn'\n",
    "fpn = 'fp'\n",
    "fnn = 'fn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp8\n"
     ]
    }
   ],
   "source": [
    "cnt = 8\n",
    "tpn += '8'\n",
    "tnn += '8'\n",
    "fpn += '8'\n",
    "fnn += '8'\n",
    "print(tpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob[tpn] = train_tp[cnt]\n",
    "train_prob[tnn] = train_tn[cnt]\n",
    "train_prob[fpn] = train_fp[cnt]\n",
    "train_prob[fnn] = train_fn[cnt]\n",
    "\n",
    "test_prob[tpn] = test_tp[cnt]\n",
    "test_prob[tnn] = test_tn[cnt]\n",
    "test_prob[fpn] = test_fp[cnt]\n",
    "test_prob[fnn] = test_fn[cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob.to_csv(r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\train_prob.csv\")\n",
    "test_prob.to_csv(r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\test_prob.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArray(input, limit):\n",
    "    for i in range(9):\n",
    "        while len(input[i]) < limit:\n",
    "            input[i].append(0)\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAsCSV(input, path):\n",
    "    new = pd.Dataframe()\n",
    "    count = 0\n",
    "    for e in effect:\n",
    "        new[e] = input[count]\n",
    "        count += 1\n",
    "    new.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = normalizeArray(test_results, 6365)\n",
    "train_results = normalizeArray(train_results, 6364)\n",
    "test_cutoffs = normalizeArray(test_cutoffs, 6365)\n",
    "train_cutoffs = normalizeArray(train_cutoffs, 6364)\n",
    "#saveAsCSV(test_results, r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\test_result.csv\")\n",
    "#saveAsCSV(train_results, r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\train_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "count = 0\n",
    "for e in effect:\n",
    "    temp[e] = test_cutoffs[count]\n",
    "    count += 1\n",
    "temp.to_csv(r\"C:\\Users\\brian\\Desktop\\Python\\Mayo Model\\test_cutoffs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56f97a71b89e05d6ccf73f1f0e2d9d243a3b4ed7741d25bc67a23080ab6402df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
